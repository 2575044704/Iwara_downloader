#!/usr/bin/env python3
"""
Iwara é«˜é€Ÿçˆ¬è™« - ä¼˜åŒ–ç‰ˆæœ¬
ä¸»è¦ä¼˜åŒ–ï¼š
1. å¢é‡ä¿å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤åºåˆ—åŒ–å·²ä¿å­˜çš„æ•°æ®
2. å¼‚æ­¥ä¿å­˜ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
3. å†…å­˜ç®¡ç†ä¼˜åŒ–ï¼Œå®šæœŸæ¸…ç†å·²ä¿å­˜çš„æ•°æ®
4. ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime
import os
import glob
from typing import List, Dict, Optional, Set
import signal
import sys
import threading
from collections import deque

# é…ç½®
TOKEN_PRIMARY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjdiM2I5MDk4LWViMjEtNGIzOC04Y2VhLTc1ZGMzM2FmMjY1YyIsInR5cGUiOiJhY2Nlc3NfdG9rZW4iLCJyb2xlIjoidXNlciIsInByZW1pdW0iOmZhbHNlLCJpc3MiOiJpd2FyYSIsImlhdCI6MTc0OTMxMzQ1MiwiZXhwIjoxNzQ5MzE3MDUyfQ.TUPlMeLik17ratOMy7XWY1aK6UeuAJoETzf2x-CsQOE"
TOKEN_BACKUP = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjA1NDVjYmNmLThkNGEtNDMxOS1iYzQ1LTVjMDJlMTNmMjIwOCIsInR5cGUiOiJhY2Nlc3NfdG9rZW4iLCJyb2xlIjoibGltaXRlZCIsInByZW1pdW0iOmZhbHNlLCJpc3MiOiJpd2FyYSIsImlhdCI6MTc0OTMxMjg3OSwiZXhwIjoxNzQ5MzE2NDc5fQ.k4sS9TKVjfv4vAmLo5njfX5MCJ4zst_JKgY2TP3RIwo"
START_PAGE = 1
END_PAGE = 8210
OUTPUT_DIR = "iwara_data"  # ä½¿ç”¨ç›®å½•å­˜å‚¨å¤šä¸ªæ–‡ä»¶
CONCURRENT_REQUESTS = 50
BATCH_SIZE = 100
SAVE_INTERVAL = 180  # 3åˆ†é’Ÿ
SAVE_EVERY_N_PAGES = 500
MEMORY_CLEAR_THRESHOLD = 1000  # æ¯1000é¡µæ¸…ç†ä¸€æ¬¡å†…å­˜

class OptimizedIwaraScraper:
    def __init__(self, resume_from: Optional[Dict] = None):
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # Tokenç®¡ç†
        self.tokens = {
            'primary': TOKEN_PRIMARY,
            'backup': TOKEN_BACKUP
        }
        self.current_token = 'primary'
        self.token_failures = {'primary': 0, 'backup': 0}
        
        self.headers = {
            'accept': 'application/json',
            'authorization': f'Bearer {self.tokens[self.current_token]}',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„
        self.pending_videos = deque()  # å¾…ä¿å­˜çš„è§†é¢‘é˜Ÿåˆ—
        self.pending_pages = deque()   # å¾…ä¿å­˜çš„é¡µé¢æ•°æ®é˜Ÿåˆ—
        self.completed_pages: Set[int] = set()  # å·²å®Œæˆçš„é¡µé¢é›†åˆ
        self.failed_pages: List[int] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_videos_saved = 0
        self.total_pages_saved = 0
        self.success_count = 0
        self.chunk_counter = 0  # ç”¨äºç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        
        # ä»æ£€æŸ¥ç‚¹æ¢å¤
        if resume_from:
            self._restore_from_checkpoint(resume_from)
        
        # æ—¶é—´ç®¡ç†
        self.start_time = time.time()
        self.last_save_time = time.time()
        self.last_save_count = 0
        self.consecutive_failures = 0
        self.is_shutting_down = False
        
        # ä¿å­˜é”ï¼Œé˜²æ­¢å¹¶å‘ä¿å­˜
        self.save_lock = asyncio.Lock()
        self.is_saving = False
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _restore_from_checkpoint(self, checkpoint: Dict):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤çŠ¶æ€"""
        print("ğŸ“¥ ä»æ£€æŸ¥ç‚¹æ¢å¤æ•°æ®...")
        metadata = checkpoint.get('metadata', {})
        
        self.completed_pages = set(metadata.get('completed_pages', []))
        self.failed_pages = metadata.get('failed_pages', [])
        self.total_videos_saved = metadata.get('total_videos_saved', 0)
        self.total_pages_saved = metadata.get('total_pages_saved', 0)
        self.success_count = metadata.get('success_count', 0)
        self.chunk_counter = metadata.get('chunk_counter', 0)
        
        print(f"   å·²æ¢å¤çŠ¶æ€: {self.total_videos_saved} ä¸ªè§†é¢‘å·²ä¿å­˜, "
              f"{self.success_count} ä¸ªé¡µé¢å·²å®Œæˆ")
    
    def _switch_token(self):
        """åˆ‡æ¢åˆ°å¤‡ç”¨Token"""
        old_token = self.current_token
        self.current_token = 'backup' if self.current_token == 'primary' else 'primary'
        self.headers['authorization'] = f'Bearer {self.tokens[self.current_token]}'
        print(f"ğŸ”„ åˆ‡æ¢Token: {old_token} â†’ {self.current_token}")
    
    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        if not self.is_shutting_down:
            self.is_shutting_down = True
            print("\n\nğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜æ•°æ®...")
            # ä½¿ç”¨çº¿ç¨‹æ‰§è¡ŒåŒæ­¥ä¿å­˜ï¼Œé¿å…åœ¨ä¿¡å·å¤„ç†å™¨ä¸­ä½¿ç”¨å¼‚æ­¥
            save_thread = threading.Thread(target=self._emergency_save_sync)
            save_thread.start()
            save_thread.join(timeout=30)  # æœ€å¤šç­‰å¾…30ç§’
            sys.exit(0)
    
    def _emergency_save_sync(self):
        """åŒæ­¥çš„ç´§æ€¥ä¿å­˜"""
        try:
            # ä¿å­˜å¾…å¤„ç†çš„æ•°æ®
            if self.pending_videos or self.pending_pages:
                filename = os.path.join(OUTPUT_DIR, f"emergency_chunk_{int(time.time())}.json")
                self._save_chunk_sync(filename, list(self.pending_videos), list(self.pending_pages))
            
            # ä¿å­˜å…ƒæ•°æ®
            self._save_metadata_sync(is_emergency=True)
            print("âœ… ç´§æ€¥ä¿å­˜å®Œæˆ")
        except Exception as e:
            print(f"âŒ ç´§æ€¥ä¿å­˜å¤±è´¥: {e}")
    
    async def fetch_page(self, session: aiohttp.ClientSession, page: int, 
                        retry_count: int = 0, token_switched: bool = False) -> Optional[Dict]:
        """è·å–å•é¡µæ•°æ®"""
        if page in self.completed_pages:
            return None
        
        url = f"https://api.iwara.tv/videos?rating=all&sort=date&page={page}"
        max_retries = 3
        
        try:
            async with session.get(url, headers=self.headers, timeout=20) as response:
                if response.status == 200:
                    data = await response.json()
                    self.success_count += 1
                    self.completed_pages.add(page)
                    self.consecutive_failures = 0
                    self.token_failures[self.current_token] = 0
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if self.success_count % 50 == 0:
                        await self._show_progress()
                    
                    return {'page': page, 'data': data}
                
                elif response.status == 429:  # é™æµ
                    self.token_failures[self.current_token] += 1
                    
                    if not token_switched and self.token_failures[self.current_token] > 5:
                        self._switch_token()
                        return await self.fetch_page(session, page, retry_count, True)
                    
                    wait_time = min(10 * (retry_count + 1), 60)
                    print(f"âš ï¸ é¡µé¢ {page} é™æµï¼Œç­‰å¾… {wait_time}ç§’...")
                    await asyncio.sleep(wait_time)
                    
                    if retry_count < max_retries:
                        return await self.fetch_page(session, page, retry_count + 1, token_switched)
                
                elif response.status == 401:  # Tokenæ— æ•ˆ
                    if not token_switched:
                        self._switch_token()
                        return await self.fetch_page(session, page, 0, True)
                    else:
                        print("âŒ ä¸¤ä¸ªTokenéƒ½å·²å¤±æ•ˆï¼")
                        self.is_shutting_down = True
                        return None
                
                else:
                    if retry_count < max_retries:
                        await asyncio.sleep(2 ** retry_count)
                        return await self.fetch_page(session, page, retry_count + 1, token_switched)
            
            self.failed_pages.append(page)
            self.consecutive_failures += 1
            return None
            
        except Exception as e:
            if retry_count < max_retries:
                await asyncio.sleep(3)
                return await self.fetch_page(session, page, retry_count + 1, token_switched)
            
            print(f"âŒ é¡µé¢ {page} å¤±è´¥: {type(e).__name__}")
            self.failed_pages.append(page)
            self.consecutive_failures += 1
            return None
    
    async def _show_progress(self):
        """æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯"""
        elapsed = time.time() - self.start_time
        rate = self.success_count / elapsed if elapsed > 0 else 0
        remaining = END_PAGE - START_PAGE - self.success_count
        eta = remaining / rate if rate > 0 else 0
        
        # è®¡ç®—å†…å­˜ä¸­å¾…ä¿å­˜çš„æ•°æ®é‡
        pending_count = len(self.pending_videos) + len(self.pending_pages)
        
        print(f"ğŸ“Š è¿›åº¦: {self.success_count}/{END_PAGE-START_PAGE+1} "
              f"é€Ÿç‡: {rate:.1f}é¡µ/ç§’ å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ "
              f"å·²ä¿å­˜: {self.total_videos_saved}è§†é¢‘ "
              f"å¾…ä¿å­˜: {pending_count}é¡¹")
    
    async def process_batch(self, session: aiohttp.ClientSession, pages: List[int]):
        """å¤„ç†ä¸€æ‰¹é¡µé¢"""
        tasks = [self.fetch_page(session, page) for page in pages]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, dict) and result:
                # æ·»åŠ åˆ°å¾…ä¿å­˜é˜Ÿåˆ—ï¼Œè€Œä¸æ˜¯ç«‹å³å­˜å‚¨åœ¨å†…å­˜ä¸­
                page_data = {
                    'page': result['page'],
                    'timestamp': datetime.now().isoformat(),
                    'data': result['data']
                }
                self.pending_pages.append(page_data)
                
                # æå–è§†é¢‘å¹¶æ·»åŠ åˆ°å¾…ä¿å­˜é˜Ÿåˆ—
                videos = result['data'].get('results', [])
                for video in videos:
                    video_copy = video.copy()
                    video_copy['_page'] = result['page']
                    video_copy['_fetchTime'] = datetime.now().isoformat()
                    self.pending_videos.append(video_copy)
    
    async def _save_chunk_async(self):
        """å¼‚æ­¥ä¿å­˜æ•°æ®å—"""
        if self.is_saving or (not self.pending_videos and not self.pending_pages):
            return
        
        async with self.save_lock:
            self.is_saving = True
            
            # è·å–è¦ä¿å­˜çš„æ•°æ®
            videos_to_save = list(self.pending_videos)
            pages_to_save = list(self.pending_pages)
            
            # æ¸…ç©ºå¾…ä¿å­˜é˜Ÿåˆ—
            self.pending_videos.clear()
            self.pending_pages.clear()
            
            # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œä¿å­˜
            loop = asyncio.get_event_loop()
            filename = os.path.join(OUTPUT_DIR, f"chunk_{self.chunk_counter:05d}.json")
            
            try:
                await loop.run_in_executor(
                    None, 
                    self._save_chunk_sync, 
                    filename, 
                    videos_to_save, 
                    pages_to_save
                )
                
                self.chunk_counter += 1
                self.total_videos_saved += len(videos_to_save)
                self.total_pages_saved += len(pages_to_save)
                
                # ä¿å­˜å…ƒæ•°æ®
                await loop.run_in_executor(None, self._save_metadata_sync, False)
                
                print(f"ğŸ’¾ ä¿å­˜æ•°æ®å— {filename}: {len(videos_to_save)} ä¸ªè§†é¢‘")
                
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
                # æ¢å¤æ•°æ®åˆ°é˜Ÿåˆ—
                self.pending_videos.extend(videos_to_save)
                self.pending_pages.extend(pages_to_save)
            
            finally:
                self.is_saving = False
    
    def _save_chunk_sync(self, filename: str, videos: List[Dict], pages: List[Dict]):
        """åŒæ­¥ä¿å­˜æ•°æ®å—"""
        data = {
            'videos': videos,
            'pages': pages,
            'metadata': {
                'chunk_id': self.chunk_counter,
                'timestamp': datetime.now().isoformat(),
                'video_count': len(videos),
                'page_count': len(pages)
            }
        }
        
        # æ™®é€šJSONä¿å­˜
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
    
    def _save_metadata_sync(self, is_emergency: bool = False):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata = {
            'total_videos_saved': self.total_videos_saved,
            'total_pages_saved': self.total_pages_saved,
            'success_count': self.success_count,
            'completed_pages': sorted(list(self.completed_pages)),
            'failed_pages': sorted(list(set(self.failed_pages))),
            'chunk_counter': self.chunk_counter,
            'duration_seconds': time.time() - self.start_time,
            'save_time': datetime.now().isoformat(),
            'is_emergency': is_emergency,
            'current_token': self.current_token,
            'token_failures': self.token_failures
        }
        
        filename = os.path.join(OUTPUT_DIR, 'metadata.json')
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def should_save(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿å­˜"""
        time_since_save = time.time() - self.last_save_time
        pages_since_save = self.success_count - self.last_save_count
        
        return (time_since_save >= SAVE_INTERVAL or
                pages_since_save >= SAVE_EVERY_N_PAGES or
                len(self.pending_videos) > 10000 or  # å†…å­˜å‹åŠ›
                self.consecutive_failures > 10)
    
    async def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("ğŸš€ Iwara ä¼˜åŒ–çˆ¬è™«å¯åŠ¨")
        print(f"ğŸ“‹ ç›®æ ‡: {START_PAGE} - {END_PAGE} (å…± {END_PAGE-START_PAGE+1} é¡µ)")
        print(f"âš™ï¸  é…ç½®: å¹¶å‘={CONCURRENT_REQUESTS}, æ‰¹æ¬¡={BATCH_SIZE}")
        print(f"ğŸ’¾ æ•°æ®ä¿å­˜åˆ°: {OUTPUT_DIR}/")
        print("ğŸ›‘ æŒ‰ Ctrl+C å®‰å…¨é€€å‡ºå¹¶ä¿å­˜\n")
        
        connector = aiohttp.TCPConnector(
            limit=CONCURRENT_REQUESTS,
            force_close=True
        )
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # æµ‹è¯•Token
            print("ğŸ”‘ éªŒè¯Token...")
            test_result = await self.fetch_page(session, START_PAGE)
            if not test_result:
                print("âŒ Tokenæ— æ•ˆæˆ–ç½‘ç»œé—®é¢˜")
                return
            print("âœ… Tokenæœ‰æ•ˆ\n")
            
            # ä¸»å¾ªç¯
            all_pages = [p for p in range(START_PAGE, END_PAGE + 1)
                        if p not in self.completed_pages]
            
            for i in range(0, len(all_pages), BATCH_SIZE):
                if self.is_shutting_down:
                    break
                
                batch = all_pages[i:i + BATCH_SIZE]
                await self.process_batch(session, batch)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
                if self.should_save():
                    await self._save_chunk_async()
                    self.last_save_time = time.time()
                    self.last_save_count = self.success_count
                
                # åŠ¨æ€è°ƒæ•´é€Ÿåº¦
                if self.consecutive_failures > 5:
                    print(f"âš ï¸ è¿ç»­å¤±è´¥{self.consecutive_failures}æ¬¡ï¼Œå‡é€Ÿ...")
                    await asyncio.sleep(5)
                else:
                    await asyncio.sleep(0.1)  # æ›´çŸ­çš„å»¶è¿Ÿ
            
            # æœ€ç»ˆä¿å­˜
            if not self.is_shutting_down:
                print("\nâœ… çˆ¬å–å®Œæˆï¼æ­£åœ¨ä¿å­˜æœ€åçš„æ•°æ®...")
                await self._save_chunk_async()
                
                # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
                await self._generate_final_report()
    
    async def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        elapsed = time.time() - self.start_time
        
        report = {
            'summary': {
                'total_duration_minutes': elapsed / 60,
                'total_videos': self.total_videos_saved,
                'total_pages': self.total_pages_saved,
                'success_pages': self.success_count,
                'failed_pages': len(set(self.failed_pages)),
                'average_rate_per_second': self.success_count / elapsed if elapsed > 0 else 0,
                'data_chunks': self.chunk_counter
            },
            'failed_pages': sorted(list(set(self.failed_pages))),
            'completion_time': datetime.now().isoformat()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(OUTPUT_DIR, 'final_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æŠ¥å‘Š
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»ç”¨æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        print(f"   æˆåŠŸé¡µæ•°: {self.success_count}")
        print(f"   å¤±è´¥é¡µæ•°: {len(set(self.failed_pages))}")
        print(f"   è§†é¢‘æ€»æ•°: {self.total_videos_saved}")
        print(f"   å¹³å‡é€Ÿç‡: {self.success_count/elapsed:.1f} é¡µ/ç§’")
        print(f"   æ•°æ®æ–‡ä»¶: {self.chunk_counter} ä¸ª")
        print(f"\nğŸ“ æ‰€æœ‰æ•°æ®ä¿å­˜åœ¨: {OUTPUT_DIR}/")

async def main():
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„å…ƒæ•°æ®
    metadata_file = os.path.join(OUTPUT_DIR, 'metadata.json')
    resume_data = None
    
    if os.path.exists(metadata_file):
        print(f"ğŸ“‚ å‘ç°ä¹‹å‰çš„å…ƒæ•°æ®: {metadata_file}")
        print("æ˜¯å¦ä»æ­¤çŠ¶æ€æ¢å¤? (y/n): ", end='')
        
        if input().strip().lower() == 'y':
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    resume_data = {'metadata': json.load(f)}
                print("âœ… çŠ¶æ€æ¢å¤æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    
    scraper = OptimizedIwaraScraper(resume_from=resume_data)
    
    try:
        await scraper.run()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {type(e).__name__}: {e}")
        scraper._emergency_save_sync()

if __name__ == "__main__":
    print("=" * 60)
    print("Iwara é«˜é€Ÿçˆ¬è™« - ä¼˜åŒ–ç‰ˆæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import aiohttp
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… aiohttp: pip install aiohttp")
        sys.exit(1)
    
    # è¿è¡Œ
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²é€€å‡º")
    except Exception as e:
        print(f"\nè‡´å‘½é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

